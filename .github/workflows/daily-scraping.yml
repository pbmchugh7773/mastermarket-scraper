name: Daily Price Scraping

on:
  # First run: 5:00 AM UTC (6:00 AM Irish time) - Full scraping
  # Second run: 14:00 UTC (3:00 PM Irish time) - Retry failed products
  schedule:
    - cron: '0 5 * * *'   # First daily run
    - cron: '0 14 * * *'  # Second daily run (retry mode)
  
  # Allow manual triggering for testing
  workflow_dispatch:
    inputs:
      max_products:
        description: 'Max products per store'
        required: false
        default: '300'
        type: string

jobs:
  scrape-prices:
    name: Scrape ${{ matrix.store }}
    runs-on: ubuntu-latest
    
    strategy:
      # Run all stores in parallel
      matrix:
        store: [Aldi, Tesco, SuperValu, Dunnes, Lidl]  # Added Lidl support

      # Don't cancel other stores if one fails
      fail-fast: false
    
    steps:
      - name: üì• Checkout Repository
        uses: actions/checkout@v4
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: üåê Install Chrome Browser
        run: |
          # Add Google Chrome repository
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          
          # Update and install Chrome and Xvfb
          sudo apt-get update
          sudo apt-get install -y \
            google-chrome-stable \
            xvfb \
            wget \
            unzip
      
      - name: üì¶ Install Python Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: üîß Setup Chrome Environment
        run: |
          # Set Chrome binary path for Selenium
          export CHROME_BIN=/usr/bin/google-chrome
          echo "CHROME_BIN=/usr/bin/google-chrome" >> $GITHUB_ENV
          
          # Get Chrome version
          CHROME_VERSION=$(google-chrome --version | sed 's/Google Chrome //' | cut -d'.' -f1-3)
          echo "Chrome version: $CHROME_VERSION"
          
          # Use Chrome for Testing ChromeDriver (more reliable for CI)
          echo "Installing ChromeDriver via webdriver-manager..."
          pip install webdriver-manager
          
          # Use dedicated script to avoid YAML multiline issues
          python3 install_chromedriver.py
          
          # Install ChromeDriver to system path
          sudo mv /tmp/chromedriver /usr/local/bin/chromedriver
          
          # Verify installations
          google-chrome --version
          chromedriver --version
          which chromedriver
      
      - name: üêõ Debug Environment
        run: |
          echo "üîç Environment Debug Information"
          echo "================================"
          google-chrome --version
          chromedriver --version
          echo "User: $(whoami)"
          echo "Working directory: $(pwd)"
          echo "Python version: $(python --version)"
          echo "Pip version: $(pip --version)"
          echo "CHROME_BIN: $CHROME_BIN"
          echo "DISPLAY: $DISPLAY"
          echo "Available display processes:"
          ps aux | grep -i xvfb || echo "No Xvfb processes found"
          echo "Environment variables:"
          env | grep -E "(CHROME|DISPLAY|API)" || echo "No relevant env vars found"
      
      - name: üõí Scrape ${{ matrix.store }} Prices
        env:
          # API Configuration
          API_URL: ${{ secrets.API_URL }}
          SCRAPER_USERNAME: ${{ secrets.SCRAPER_USERNAME }}
          SCRAPER_PASSWORD: ${{ secrets.SCRAPER_PASSWORD }}
          
          # Chrome Configuration
          CHROME_BIN: /usr/bin/google-chrome
          DISPLAY: :99
          
          # Store Configuration
          STORE_NAME: ${{ matrix.store }}
          MAX_PRODUCTS: ${{ github.event.inputs.max_products || '300' }}
        
        run: |
          echo "üè™ Starting ${{ matrix.store }} scraping..."
          echo "üìä Max products: $MAX_PRODUCTS"
          echo "üïê Started at: $(date)"

          # Detect if this is a retry run (second execution of the day)
          CURRENT_HOUR=$(date -u +%H)
          if [ "$CURRENT_HOUR" -ge "14" ]; then
            echo "üîÑ RETRY MODE: Processing only failed/pending products from morning run"
            RETRY_FLAG="--retry-mode"
            RUN_TYPE="retry"
          else
            echo "üì¶ NORMAL MODE: Processing all products (first run of the day)"
            RETRY_FLAG=""
            RUN_TYPE="normal"
          fi

          # Start virtual display for headless Chrome
          Xvfb :99 -screen 0 1920x1080x24 > /dev/null 2>&1 &

          # Run the scraper with appropriate mode
          python simple_local_to_prod.py \
            --store "$STORE_NAME" \
            --products "$MAX_PRODUCTS" \
            $RETRY_FLAG

          echo "‚úÖ ${{ matrix.store }} scraping ($RUN_TYPE mode) completed at: $(date)"
      
      - name: üìä Upload Results Summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraping-logs-${{ matrix.store }}-${{ github.run_number }}
          path: |
            *.log
            logs/
          retention-days: 7

  # Summary job that runs after all stores complete
  scraping-summary:
    name: Scraping Summary
    needs: scrape-prices
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: üìä Generate Summary
        run: |
          echo "üéØ Daily Scraping Summary - $(date)"
          echo "=================================="
          
          # Check results from matrix jobs
          if [[ "${{ needs.scrape-prices.result }}" == "success" ]]; then
            echo "‚úÖ All stores completed successfully"
          elif [[ "${{ needs.scrape-prices.result }}" == "failure" ]]; then
            echo "‚ùå Some stores failed - check individual job logs"
          else
            echo "‚ö†Ô∏è Mixed results - check individual job logs"
          fi
          
          echo ""

          # Determine run type
          CURRENT_HOUR=$(date -u +%H)
          if [ "$CURRENT_HOUR" -ge "14" ]; then
            echo "üìà Expected Results (RETRY MODE):"
            echo "‚Ä¢ Only processes products that failed in morning run"
            echo "‚Ä¢ Typical retry success rate: 60-80%"
            echo "‚Ä¢ Combined daily success rate target: 95%+"
          else
            echo "üìà Expected Results (NORMAL MODE):"
            echo "‚Ä¢ Aldi: ~300 products in ~30 minutes"
            echo "‚Ä¢ Tesco: ~300 products in ~10 hours (with promotions!)"
            echo "‚Ä¢ SuperValu: ~300 products in ~10 hours"
            echo "‚Ä¢ Dunnes: ~300 products in ~70 minutes"
            echo "‚Ä¢ Lidl: ~300 products in ~30 minutes"
            echo "‚Ä¢ Total: ~1500 products updated"
            echo "‚Ä¢ Failed products will be retried at 14:00 UTC"
          fi
          echo ""
          echo "üîó Check results at: https://www.mastermarketapp.com/products"
      
      - name: üö® Notify on Failure
        if: needs.scrape-prices.result == 'failure'
        run: |
          echo "‚ö†Ô∏è SCRAPING FAILURE DETECTED"
          echo "Please check the individual job logs for details."
          echo "Common causes:"
          echo "‚Ä¢ API authentication issues"
          echo "‚Ä¢ Website anti-bot measures"
          echo "‚Ä¢ Network connectivity problems"
          echo "‚Ä¢ Chrome/Selenium configuration issues"
          echo ""
          echo "üîß Troubleshooting:"
          echo "‚Ä¢ Verify GitHub Secrets are set correctly"
          echo "‚Ä¢ Check MasterMarket API status"
          echo "‚Ä¢ Review individual store job logs"
          echo "‚Ä¢ Consider manual re-run if temporary issue"