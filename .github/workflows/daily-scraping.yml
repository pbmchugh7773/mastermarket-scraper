name: Daily Price Scraping

on:
  # First run: 5:00 AM UTC (6:00 AM Irish time) - Full scraping
  # Second run: 14:00 UTC (3:00 PM Irish time) - Retry failed products
  # Third run: 8:00 AM UTC on Sundays (9:00 AM Irish time) - Weekly promotions scan
  schedule:
    - cron: '0 5 * * *'   # Daily run (normal mode)
    - cron: '0 14 * * *'  # Daily retry run (retry mode)
    - cron: '0 8 * * 0'   # Weekly promotions run (promotions mode) - Sundays
  
  # Allow manual triggering for testing
  workflow_dispatch:
    inputs:
      max_products:
        description: 'Max products per store'
        required: false
        default: '300'
        type: string
      promotions_mode:
        description: 'Enable promotions mode (captures promotional data)'
        required: false
        default: false
        type: boolean

jobs:
  scrape-prices:
    name: Scrape ${{ matrix.store }}
    runs-on: ubuntu-latest
    
    strategy:
      # Run all stores in parallel
      matrix:
        store: [Aldi, Tesco, SuperValu, Dunnes, Lidl]  # Added Lidl support

      # Don't cancel other stores if one fails
      fail-fast: false
    
    steps:
      - name: üì• Checkout Repository
        uses: actions/checkout@v4
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: üåê Install Chrome Browser
        run: |
          # Add Google Chrome repository
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          
          # Update and install Chrome and Xvfb
          sudo apt-get update
          sudo apt-get install -y \
            google-chrome-stable \
            xvfb \
            wget \
            unzip
      
      - name: üì¶ Install Python Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: üîß Setup Chrome Environment
        run: |
          # Set Chrome binary path for Selenium
          export CHROME_BIN=/usr/bin/google-chrome
          echo "CHROME_BIN=/usr/bin/google-chrome" >> $GITHUB_ENV
          
          # Get Chrome version
          CHROME_VERSION=$(google-chrome --version | sed 's/Google Chrome //' | cut -d'.' -f1-3)
          echo "Chrome version: $CHROME_VERSION"
          
          # Use Chrome for Testing ChromeDriver (more reliable for CI)
          echo "Installing ChromeDriver via webdriver-manager..."
          pip install webdriver-manager
          
          # Use dedicated script to avoid YAML multiline issues
          python3 install_chromedriver.py
          
          # Install ChromeDriver to system path
          sudo mv /tmp/chromedriver /usr/local/bin/chromedriver
          
          # Verify installations
          google-chrome --version
          chromedriver --version
          which chromedriver
      
      - name: üêõ Debug Environment
        run: |
          echo "üîç Environment Debug Information"
          echo "================================"
          google-chrome --version
          chromedriver --version
          echo "User: $(whoami)"
          echo "Working directory: $(pwd)"
          echo "Python version: $(python --version)"
          echo "Pip version: $(pip --version)"
          echo "CHROME_BIN: $CHROME_BIN"
          echo "DISPLAY: $DISPLAY"
          echo "Available display processes:"
          ps aux | grep -i xvfb || echo "No Xvfb processes found"
          echo "Environment variables:"
          env | grep -E "(CHROME|DISPLAY|API)" || echo "No relevant env vars found"
      
      - name: üõí Scrape ${{ matrix.store }} Prices
        env:
          # API Configuration
          API_URL: ${{ secrets.API_URL }}
          SCRAPER_USERNAME: ${{ secrets.SCRAPER_USERNAME }}
          SCRAPER_PASSWORD: ${{ secrets.SCRAPER_PASSWORD }}
          
          # Chrome Configuration
          CHROME_BIN: /usr/bin/google-chrome
          DISPLAY: :99
          
          # Store Configuration
          STORE_NAME: ${{ matrix.store }}
          MAX_PRODUCTS: ${{ github.event.inputs.max_products || '300' }}
          MANUAL_PROMOTIONS_MODE: ${{ github.event.inputs.promotions_mode || 'false' }}
        
        run: |
          echo "üè™ Starting ${{ matrix.store }} scraping..."
          echo "üìä Max products: $MAX_PRODUCTS"
          echo "üïê Started at: $(date)"

          # Detect execution mode based on time, day, and manual inputs
          CURRENT_HOUR=$(date -u +%H)
          CURRENT_DAY=$(date -u +%u)  # 1=Monday, 7=Sunday

          if [ "$MANUAL_PROMOTIONS_MODE" = "true" ]; then
            echo "üéØ PROMOTIONS MODE: Manual promotions scan triggered"
            EXTRA_FLAGS="--promotions-mode"
            RUN_TYPE="promotions"
          elif [ "$CURRENT_DAY" -eq "7" ] && [ "$CURRENT_HOUR" -eq "8" ]; then
            echo "üéØ PROMOTIONS MODE: Weekly comprehensive promotions scan (Sundays 8:00 UTC)"
            EXTRA_FLAGS="--promotions-mode"
            RUN_TYPE="promotions"
          elif [ "$CURRENT_HOUR" -ge "14" ]; then
            echo "üîÑ RETRY MODE: Processing only failed/pending products from morning run"
            EXTRA_FLAGS="--retry-mode"
            RUN_TYPE="retry"
          else
            echo "üì¶ NORMAL MODE: Processing all products (first run of the day)"
            EXTRA_FLAGS=""
            RUN_TYPE="normal"
          fi

          # Start virtual display for headless Chrome
          Xvfb :99 -screen 0 1920x1080x24 > /dev/null 2>&1 &

          # Run the scraper with appropriate mode
          python simple_local_to_prod.py \
            --store "$STORE_NAME" \
            --products "$MAX_PRODUCTS" \
            $EXTRA_FLAGS

          echo "‚úÖ ${{ matrix.store }} scraping ($RUN_TYPE mode) completed at: $(date)"
      
      - name: üìä Upload Results Summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraping-logs-${{ matrix.store }}-${{ github.run_number }}
          path: |
            *.log
            logs/
          retention-days: 7

  # Summary job that runs after all stores complete
  scraping-summary:
    name: Scraping Summary
    needs: scrape-prices
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: üìä Generate Summary
        run: |
          echo "üéØ Daily Scraping Summary - $(date)"
          echo "=================================="
          
          # Check results from matrix jobs
          if [[ "${{ needs.scrape-prices.result }}" == "success" ]]; then
            echo "‚úÖ All stores completed successfully"
          elif [[ "${{ needs.scrape-prices.result }}" == "failure" ]]; then
            echo "‚ùå Some stores failed - check individual job logs"
          else
            echo "‚ö†Ô∏è Mixed results - check individual job logs"
          fi
          
          echo ""

          # Determine run type for summary
          CURRENT_HOUR=$(date -u +%H)
          CURRENT_DAY=$(date -u +%u)  # 1=Monday, 7=Sunday

          # Check if this was a manual promotions run from the environment
          if echo "${{ needs.scrape-prices.outputs.run_type || '' }}" | grep -q "promotions" || [ "$CURRENT_DAY" -eq "7" ] && [ "$CURRENT_HOUR" -eq "8" ]; then
            echo "üìà Expected Results (PROMOTIONS MODE):"
            echo "‚Ä¢ Comprehensive promotions scan for all stores"
            echo "‚Ä¢ Captures Clubcard prices, multi-buy deals, discounts"
            echo "‚Ä¢ Updates promotion_type, promotion_text, original_price fields"
            echo "‚Ä¢ Tesco: Enhanced Clubcard price detection"
            echo "‚Ä¢ All stores: Full promotional metadata capture"
            echo "‚Ä¢ Weekly promotions database refresh"
          elif [ "$CURRENT_HOUR" -ge "14" ]; then
            echo "üìà Expected Results (RETRY MODE):"
            echo "‚Ä¢ Only processes products that failed in morning run"
            echo "‚Ä¢ Typical retry success rate: 60-80%"
            echo "‚Ä¢ Combined daily success rate target: 95%+"
          else
            echo "üìà Expected Results (NORMAL MODE):"
            echo "‚Ä¢ Aldi: ~300 products in ~30 minutes"
            echo "‚Ä¢ Tesco: ~300 products in ~10 hours (with promotions!)"
            echo "‚Ä¢ SuperValu: ~300 products in ~10 hours"
            echo "‚Ä¢ Dunnes: ~300 products in ~70 minutes"
            echo "‚Ä¢ Lidl: ~300 products in ~30 minutes"
            echo "‚Ä¢ Total: ~1500 products updated"
            echo "‚Ä¢ Failed products will be retried at 14:00 UTC"
          fi
          echo ""
          echo "üîó Check results at: https://www.mastermarketapp.com/products"
      
      - name: üö® Notify on Failure
        if: needs.scrape-prices.result == 'failure'
        run: |
          echo "‚ö†Ô∏è SCRAPING FAILURE DETECTED"
          echo "Please check the individual job logs for details."
          echo "Common causes:"
          echo "‚Ä¢ API authentication issues"
          echo "‚Ä¢ Website anti-bot measures"
          echo "‚Ä¢ Network connectivity problems"
          echo "‚Ä¢ Chrome/Selenium configuration issues"
          echo ""
          echo "üîß Troubleshooting:"
          echo "‚Ä¢ Verify GitHub Secrets are set correctly"
          echo "‚Ä¢ Check MasterMarket API status"
          echo "‚Ä¢ Review individual store job logs"
          echo "‚Ä¢ Consider manual re-run if temporary issue"