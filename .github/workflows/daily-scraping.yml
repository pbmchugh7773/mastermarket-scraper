name: Daily Price Scraping

on:
  # Run daily at 5:00 AM UTC (6:00 AM Irish time)
  schedule:
    - cron: '0 5 * * *'
  
  # Allow manual triggering for testing
  workflow_dispatch:
    inputs:
      max_products:
        description: 'Max products per store'
        required: false
        default: '67'
        type: string

jobs:
  scrape-prices:
    name: Scrape ${{ matrix.store }}
    runs-on: ubuntu-latest
    
    strategy:
      # Run all stores in parallel
      matrix:
        store: [Aldi, Tesco, SuperValu, Dunnes]  # Re-enabled Dunnes with improved handling
      
      # Don't cancel other stores if one fails
      fail-fast: false
    
    steps:
      - name: üì• Checkout Repository
        uses: actions/checkout@v4
      
      - name: üêç Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: üåê Install Chrome Browser
        run: |
          # Add Google Chrome repository
          wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          
          # Update and install Chrome and Xvfb
          sudo apt-get update
          sudo apt-get install -y \
            google-chrome-stable \
            xvfb \
            wget \
            unzip
      
      - name: üì¶ Install Python Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: üîß Setup Chrome Environment
        run: |
          # Set Chrome binary path for Selenium
          export CHROME_BIN=/usr/bin/google-chrome
          echo "CHROME_BIN=/usr/bin/google-chrome" >> $GITHUB_ENV
          
          # Get Chrome version
          CHROME_VERSION=$(google-chrome --version | sed 's/Google Chrome //' | cut -d'.' -f1-3)
          echo "Chrome version: $CHROME_VERSION"
          
          # Use Chrome for Testing ChromeDriver (more reliable for CI)
          # Simplified approach to avoid YAML multiline issues
          echo "Installing ChromeDriver via webdriver-manager..."
          pip install webdriver-manager
          
          # Use webdriver-manager for reliable ChromeDriver installation
          python3 -c "
from webdriver_manager.chrome import ChromeDriverManager
import os, shutil, glob

# Get the base directory path from webdriver-manager
base_path = ChromeDriverManager().install()
print(f'Base path from webdriver-manager: {base_path}')

# Find the actual chromedriver executable
if os.path.isdir(base_path):
    # Look for chromedriver executable (not THIRD_PARTY_NOTICES)
    chromedriver_files = []
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file == 'chromedriver' and not file.endswith('.chromedriver'):
                full_path = os.path.join(root, file)
                # Verify it's an executable, not a text file
                if os.path.isfile(full_path) and os.access(full_path, os.X_OK):
                    chromedriver_files.append(full_path)
    
    if chromedriver_files:
        driver_path = chromedriver_files[0]
    else:
        print('No executable chromedriver found, trying alternative locations...')
        # Try parent directory
        parent_dir = os.path.dirname(base_path)
        alt_paths = [
            os.path.join(parent_dir, 'chromedriver-linux64', 'chromedriver'),
            os.path.join(os.path.dirname(parent_dir), 'chromedriver-linux64', 'chromedriver')
        ]
        driver_path = next((p for p in alt_paths if os.path.isfile(p) and os.access(p, os.X_OK)), None)
        
        if not driver_path:
            print(f'Error: No executable chromedriver found in {base_path} or alternatives')
            exit(1)
else:
    # base_path is a file, check if it's the actual chromedriver
    if base_path.endswith('chromedriver') and not 'THIRD_PARTY' in base_path:
        driver_path = base_path
    else:
        print(f'Error: Invalid chromedriver path: {base_path}')
        exit(1)

print(f'Using ChromeDriver at: {driver_path}')

# Verify the file is executable
if os.path.exists(driver_path) and os.access(driver_path, os.X_OK):
    shutil.copy2(driver_path, '/tmp/chromedriver')
    os.chmod('/tmp/chromedriver', 0o755)
    print('ChromeDriver copied successfully')
else:
    print(f'Error: ChromeDriver not found or not executable at {driver_path}')
    exit(1)
"
          
          # Install ChromeDriver to system path
          sudo mv /tmp/chromedriver /usr/local/bin/chromedriver
          
          # Verify installations
          google-chrome --version
          chromedriver --version
          which chromedriver
      
      - name: üêõ Debug Environment
        run: |
          echo "üîç Environment Debug Information"
          echo "================================"
          google-chrome --version
          chromedriver --version
          echo "User: $(whoami)"
          echo "Working directory: $(pwd)"
          echo "Python version: $(python --version)"
          echo "Pip version: $(pip --version)"
          echo "CHROME_BIN: $CHROME_BIN"
          echo "DISPLAY: $DISPLAY"
          echo "Available display processes:"
          ps aux | grep -i xvfb || echo "No Xvfb processes found"
          echo "Environment variables:"
          env | grep -E "(CHROME|DISPLAY|API)" || echo "No relevant env vars found"
      
      - name: üõí Scrape ${{ matrix.store }} Prices
        env:
          # API Configuration
          API_URL: ${{ secrets.API_URL }}
          SCRAPER_USERNAME: ${{ secrets.SCRAPER_USERNAME }}
          SCRAPER_PASSWORD: ${{ secrets.SCRAPER_PASSWORD }}
          
          # Chrome Configuration
          CHROME_BIN: /usr/bin/google-chrome
          DISPLAY: :99
          
          # Store Configuration
          STORE_NAME: ${{ matrix.store }}
          MAX_PRODUCTS: ${{ github.event.inputs.max_products || '67' }}
        
        run: |
          echo "üè™ Starting ${{ matrix.store }} scraping..."
          echo "üìä Max products: $MAX_PRODUCTS"
          echo "üïê Started at: $(date)"
          
          # Start virtual display for headless Chrome
          Xvfb :99 -screen 0 1920x1080x24 > /dev/null 2>&1 &
          
          # Run the scraper
          python simple_local_to_prod.py \
            --store "$STORE_NAME" \
            --products "$MAX_PRODUCTS"
          
          echo "‚úÖ ${{ matrix.store }} scraping completed at: $(date)"
      
      - name: üìä Upload Results Summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraping-logs-${{ matrix.store }}-${{ github.run_number }}
          path: |
            *.log
            logs/
          retention-days: 7

  # Summary job that runs after all stores complete
  scraping-summary:
    name: Scraping Summary
    needs: scrape-prices
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: üìä Generate Summary
        run: |
          echo "üéØ Daily Scraping Summary - $(date)"
          echo "=================================="
          
          # Check results from matrix jobs
          if [[ "${{ needs.scrape-prices.result }}" == "success" ]]; then
            echo "‚úÖ All stores completed successfully"
          elif [[ "${{ needs.scrape-prices.result }}" == "failure" ]]; then
            echo "‚ùå Some stores failed - check individual job logs"
          else
            echo "‚ö†Ô∏è Mixed results - check individual job logs"
          fi
          
          echo ""
          echo "üìà Expected Results:"
          echo "‚Ä¢ Aldi: ~67 products in ~6 minutes"  
          echo "‚Ä¢ Tesco: ~67 products in ~2.2 hours"
          echo "‚Ä¢ SuperValu: ~67 products in ~2.2 hours"
          echo "‚Ä¢ Dunnes: ~67 products in ~15 minutes"
          echo "‚Ä¢ Total: ~268 products updated"
          echo ""
          echo "üîó Check results at: https://www.mastermarketapp.com/products"
      
      - name: üö® Notify on Failure
        if: needs.scrape-prices.result == 'failure'
        run: |
          echo "‚ö†Ô∏è SCRAPING FAILURE DETECTED"
          echo "Please check the individual job logs for details."
          echo "Common causes:"
          echo "‚Ä¢ API authentication issues"
          echo "‚Ä¢ Website anti-bot measures"
          echo "‚Ä¢ Network connectivity problems"
          echo "‚Ä¢ Chrome/Selenium configuration issues"
          echo ""
          echo "üîß Troubleshooting:"
          echo "‚Ä¢ Verify GitHub Secrets are set correctly"
          echo "‚Ä¢ Check MasterMarket API status"
          echo "‚Ä¢ Review individual store job logs"
          echo "‚Ä¢ Consider manual re-run if temporary issue"